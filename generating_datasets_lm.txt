While pretrained language models (PLMs) achieve
strong results for many NLP tasks (Peters et al.,
2018; Radford et al., 2018; Devlin et al., 2019),
they do not produce good sentence embeddings out
of the box (Reimers and Gurevych, 2019). Recent
approaches address this by augmenting or replacing
the language modeling objective with likewise un-
supervised sentence-level objectives (e.g., Zhang
et al., 2020; Li et al., 2020), but they typically
lag behind their supervised counterparts trained on
human-annotated sentence pairs. Unfortunately,
obtaining large amounts of high-quality training
data can be both difficult and prohibitively expen-
sive (Bowman et al., 2015; Agirre et al., 2016).
Furthermore, with larger and larger model sizes
(Radford et al., 2019; Raffel et al., 2020; Brown
et al., 2020; Fedus et al., 2021), it becomes increas-
ingly challenging to finetune PLMs.
1Our code and datasets are publicly available at https:
//github.com/timoschick/dino .Task : Write two sentences that mean the same thing.
Sentence 1 : “A man is playing a flute.”
Sentence 2 : “He’s playing a flute.”
Task : Write two sentences that are somewhat similar.
Sentence 1 : “A man is playing a flute.”
Sentence 2 : “A woman has been playing the violin.”
Task : Write two sentences that are on completely
different topics.
Sentence 1 : “A man is playing a flute.”
Sentence 2 : “A woman is walking down the street.”
Figure 1: Continuations generated by GPT2-XL with
DINO for three different task descriptions. We investi-
gate two different unsupervised approaches to generat-
ing sentence-similarity datasets: (i) The input sentence
is given and only the continuation is generated. This
requires that an (unlabeled) set of sentences is available.
(ii) Both input sentence and continuation are generated.
This does not rely on the availability of any resources.
To alleviate both problems, we explore a novel
approach to obtaining high-quality sentence em-
beddings: We mimic the creation of NLI datasets
by human crowdworkers (Bowman et al., 2015;
Williams et al., 2018), but replace human annota-
tors with large PLMs. This allows us to automat-
ically create entire datasets from scratch that can
be used for supervised training of much smaller
models. Not only does this solve the problem of
limited training data, it also provides a viable path
to leverage big models like GPT-3 (Brown et al.,
2020) without requiring any updates to their param-
eters. As illustrated in Figure 1, our approach is
based on recent methods for providing instructions
to PLMs (e.g., Radford et al., 2019; Brown et al.,
2020; Schick and Schütze, 2020, 2021a). We use
the self-debiasing approach of Schick et al. (2021)
to ensure that each generated text pair is not only agood fit for a given similarity label, but also not a
good fit for other labels. We refer to our method as
Datasets from Instructi ons (D INO ).
In summary, our contributions are as follows:
•We introduce DINO , a method for automati-
cally generating labeled datasets of arbitrary
size by providing PLMs with instructions.
•We release STS -
 (read as “STS-Dino”), the
first textual similarity dataset generated com-
pletely automatically, without any human an-
notation effort.
•We show that Sentence-RoBERTa (Reimers
and Gurevych, 2019) trained on STS -
 out-
performs strong baselines on several semantic
textual similarity datasets.
2 Related Work
There are many unsupervised approaches to ob-
taining sentence embeddings, for example by av-
eraging word embeddings (Mikolov et al., 2013;
Pennington et al., 2014; Bojanowski et al., 2017) or
with carefully designed sentence-level objectives
(Le and Mikolov, 2014; Kiros et al., 2015). Ensem-
bling several methods improves results (Pörner and
Schütze, 2019; Pörner et al., 2020). Recent work
obtains sentence representations by supplementing
BERT (Devlin et al., 2019) or other PLMs with
additional unsupervised objectives (Zhang et al.,
2020; Li et al., 2020; Wu et al., 2020; Giorgi et al.,
2020). Often, labeled datasets such as paraphrase
databases (Wieting and Gimpel, 2018) or natural
language inference datasets (Conneau et al., 2017;
Cer et al., 2018; Reimers and Gurevych, 2019) are
used for supervised learning.
Some approaches augment existing datasets with
automatically generated examples (Anaby-Tavor
et al., 2020; Papanikolaou and Pierleoni, 2020;
Yang et al., 2020; Mohapatra et al., 2020; Kumar
et al., 2021), but in contrast to our work, all of
these approaches require that there already exists a
labeled dataset for finetuning the generator. Provid-
ing PLMs with task descriptions for zero- or few-
shot learning has been studied extensively (e.g.,
Radford et al., 2019; Puri and Catanzaro, 2019;
Brown et al., 2020; Schick and Schütze, 2020,
2021b,a; Weller et al., 2020; Gao et al., 2021; Tam
et al., 2021). However, none of these approaches is
suitable for generating sentence embeddings.
Closely related to our work, Efrat and Levy
(2020) examine the ability of PLMs to follow natu-Task : Write two sentences that iy.
Sentence 1 : “x1”
Sentence 2 : “
Figure 2: Instruction template Iy(x1)for similarity la-
belyand input sentence x1;iyis described in Section 3.
See Figure 1 for three instantiations of the template.
ral language instructions for generating examples
in place of human crowdworkers, but find that their
approach performs poorly.
3 Datasets from Instructions
Let M be a PLM with vocabulary V,X =V∗
the set of all token sequences and Ya finite set of
semantic similarity labels. Our aim is to generate a
dataset Z⊂X×X×Yof text pairs (x1,x2)with
corresponding similarity labels y. For x∈Vand
x∈X, we denote with pM(x|x)the probability
that M assigns to xas a continuation of x.
We first assume that we already have access
to a set X1⊂Xof texts (e.g., a set of sentences
that are typical of the domain of interest). This is a
realistic setting for many real-world applications,
where large amounts of unlabeled text are abundant,
but it is difficult to obtain interesting and (for our
task) useful text pairs and labels. DINO requires a
set of instructions I={Iy|y∈Y}where each
Iy∈ I is a function that, given an input x1∈X1,
prompts its recipient to generate an appropriate
second text x2. We use the instruction template
in Figure 2 and consider three levels of similarity
(Y={0,0.5,1}), where
iy=

mean the same thing ify= 1
are somewhat similar ify= 0.5
are on completely different topics ify= 0
is loosely based on Cer et al. (2017)’s five-level
similarity scheme. Note that for all y,Iyends with
an opening quotation mark, which allows us to treat
the first quotation mark generated by the PLM as a
sign that it is done.
For a given x1∈X1and y∈Y, we could
directly use the instructions Iyto obtain x2by con-
tinuously sampling tokens
xk∼pM(xk|Iy(x1), x 1, . . . , x k−1)
starting from k= 1 until xkis a quotation mark
and setting x2=x1, . . . , x k−1. However, we maywant the PLM to generate a text x2that is not only
a good fit for instruction Iy(x1), but also not a
good fit for some other instruction Iy′(x1). We
refer to y′as a counterlabel for yand denote the
set of y’s counterlabels as CL(y). For example,
1∈CL(0.5)means that for y= 0 .5, we want
M to generate a sentence x2that is similar to
(y= 0 .5), but at the same time does not have the
same meaning as ( y= 1 ) sentence x1. We achieve
this using Schick et al. (2021)’s self-debiasing algo-
rithm: When sampling the token xk, we consider
not just py=pM(xk|Iy(x1), x 1, . . . , x k−1)[xk’s
probability given Iy(x1)], but also py′[xk’s prob-
ability given Iy′(x1)], for all y′∈CL(y). We
penalize each token xkfor which pyis lower than
any py′by multiplying its probability with a factor
α= exp( λ·δy)where
δy=py− max
y′∈CL (y)py′
is the difference between xk’s probability given
Iy(x1)and its maximum probability given Iy′(x1)
for any y′∈CL(y), and the decay constant λis a
hyperparameter.
For settings where no set of unlabeled texts X1
is available , a straightforward approach would be
to use the phrase shown in Figure 2 up to and in-
cluding the first quotation mark as an instruction
to let the PLM generate both x1and x2. However,
this approach has at least two issues: First, gener-
ated texts may not match the required schema (e.g.,
the model may never produce the string “Sentence
2:”). Second, the set of texts x1should ideally be
highly diverse, whereas we want to give the model
less leeway when generating x2, so we may want
to use different sampling strategies for x1and x2.
We solve both problems as follows: We first use
Iy(Figure 2) up to and including the first quotation
mark (the one right after “Sentence 1:”) to generate
x1; we stop as soon as the model produces a quota-
tion mark. We run this procedure repeatedly until
we have a sufficient number of sentences. These
are gathered into a set X1and then we proceed
exactly as in the case where X1is already given.
4 Experiments
We evaluate DINO on several English semantic tex-
tual similarity datasets: the STS tasks 2012–2016
(Agirre et al., 2012, 2013, 2014, 2015, 2016), the
STS benchmark (STSb) (Cer et al., 2017), and the
SICK-Relatedness dataset (SICK) (Marelli et al.,2014). For all tasks, we adopt the unsupervised
setting without task-specific training examples.
We use DINO to generate STS -
⊂X×X×Y, a
dataset of text pairs with semantic similarity labels.
We generate two variants:
•STS -
-x2, for which we make use of STSb
to obtain a set of texts X1;
•STS -
-x1x2, where the set of sentences X1
is generated from scratch.
We use GPT2-XL as PLM with a decay constant
ofλ= 100 and the set of counterlabels CL(y) =
{y′∈Y|y′> y}. That is, we do not restrict
the PLM when generating texts for y= 1 , but for
y= 0 .5(y= 0 ) we encourage it not to generate
texts x2that mean the same thing as (are somewhat
similar to) x1. We apply top- p(Holtzman et al.,
2020) and top- k(Fan et al., 2018; Holtzman et al.,
2018) sampling with p= 0 .9,k= 5 and generate
up to 40 output tokens. For each x1∈X1and y∈
Y, we generate up to two corresponding x2’s.2For
STS -
-x1x2, we obtain X1by generating 15,000
sentences using only top- psampling (again with
p= 0 .9) and no top- ksampling to ensure more
diversity in the generated output. We remove all
examples where x1=x2(as those provide no
training signal to the model) and split the datasets
90/10 into training and validation.
To assess the quality of the generated datasets,
we use them to train Sentence-RoBERTa (Reimers
and Gurevych, 2019), a biencoder architecture
based on RoBERTa (base) (Liu et al., 2019) that
measures the similarity of two texts by comput-
ing the cosine similarity of their embeddings. As
our datasets contain many noisy examples, we use
a technique similar to label smoothing (Szegedy
et al., 2016) and replace similarity scores of 0and
1with 0.1and 0.9, respectively. Additionally, for
each x1, we sample two x2’s from other dataset
entries and augment the dataset with (x1,x2,0).
We use the default parameters of Reimers and
Gurevych (2019) with a batch size of 32 and train
for at most one epoch; the exact number of train-
ing steps is determined based on Spearman’s rank
correlation on the STS-
 validation set.
Results We compare S-RoBERTa (base) trained
on datasets generated with DINO to S-BERT and
S-RoBERTa finetuned on NLI data as well as Uni-
versal Sentence Encoder (USE) (Cer et al., 2018)
and InferSent (Conneau et al., 2017), all of which
are trained on hundreds of thousands of labeled text
pairs from SNLI (Bowman et al., 2015) and MNLI
(Williams et al., 2018). We additionally compare
to the following fully unsupervised approaches: av-
eraging word-level GloVe (Pennington et al., 2014)
or BERT (Devlin et al., 2019) embeddings, using
BERT’s CLS token, and recent methods by Zhang
et al. (2020) and Li et al. (2020) based on pretrained
BERT models. We do not compare to approaches
trained with direct supervision as our focus is on
obtaining sentence representations without task-
specific labeled examples. As shown in Table 1,
training on datasets generated with DINO clearly
outperforms the fully unsupervised baselines; on
average, training on STS -
-x2even outperforms
all approaches with NLI supervision. STS -
-x2
gives better results than STS -
-x1x2on all STS
datasets as its examples are – by design – very sim-
ilar to examples found in these datasets, while the
latter gives better results on SICK.We investigate the importance of self-debiasing
(Schick et al., 2021) in Table 2 (top); as can be
seen, removing self-debiasing ( λ= 0 ) dramatically
hurts performance. Increasing the decay constant
(λ= 200 ) leads to slightly worse performance
as the overall quality of generated sentences de-
creases (Schick et al., 2021). Table 2 (bottom)
shows that training on STS -
 requires measures
to limit the effect of noisy labels: removing label
smoothing and performing no data augmentation
(i.e., not generating additional pairs (x1,x2,0)by
sampling random x2’s for each x1) clearly hurts
performance.
To further assess the quality of datasets gener-
ated with DINO , we additionally perform a small-
scale human evaluation. To this end, we consider
the exact version of STS -
-x2used for training
S-RoBERTa; that is, we perform label smoothing,
augmentation with randomly sampled text pairs,
and removal of trivial examples where x1=x2.
From the resulting dataset, we randomly select
100 text pairs (x1,x2)and annotate them ourselves
with similarity scores y∈ { 0,0.1,0.5,0.9}, where
we assign a score of 0.9when x1and x2mean
(almost) the same thing and a score of 0.1when
they are on different topics, but still show a weak
similarity in some aspect.
In Table 3, human annotations are compared
to originally assigned scores, yielding some inter-
esting insights. For one, it becomes clear why
augmentation with randomly sampled text pairs is
important for good downstream task performance:
Of the examples generated by DINO that are sup-DINO Labels → 0.0 0 .1 0 .5 0 .9Human Labels0.0 95% 15% 0% 0%
0.1 0% 44% 11% 12%
0.5 5% 41% 60% 41%
0.9 0% 0% 29% 47%
Table 3: Comparison of similarity scores in STS -
-x2
to human judgments for 100 examples. Examples are
chosen randomly from the version of STS -
-x2used
for training (including label smoothing, augmentation
with random pairs and removal of examples where
x1=x2). For column iand row j, the value shown is
the percentage of examples generated by DINO for sim-
ilarity score ithat were assigned score jin our human
evaluation.
posed to be on completely different topics, many
(41%) still have a certain similarity according to
human judgment. In contrast, randomly sampled
pairs are indeed on completely different topics in
almost all cases. Moreover, we can see that GPT2-
XL has particular difficulty in generating pairs of
non-identical sentences that really mean the same
thing: Only 47% of all examples that should have
the same meaning do actually mean (almost) the
same thing. However, the strong performance of
S-RoBERTa trained on STS -
-x2suggests that,
despite this noise, there is sufficient signal in this
dataset for successful training.
We finally take a qualitative look at both positive
examples where DINO is able to create high-quality
text pairs and at some typical errors found in many
of the generated examples. As shown in Table 4, for
y= 1 the PLM sometimes comes up with decent
paraphrases (e.g. “notches a victory” 7→“wins”) or
substitutes with very similar meaning (“cutting” 7→
“slicing”), but more often it generates sentences that
either omit or mix up important information, and
sometimes it produces sentences with an entirely
different meaning. Whereas sentences generated
fory= 0 .5by and large look reasonable, for y= 0
the PLM often simply flips words (“closed” 7→
“open”, “large” 7→ “small”) instead of producing
sentences on completely different topics.
5 Conclusion
We have introduced DINO , a method for using large
PLMs to generate entire datasets of labeled sen-
tence pairs from scratch, requiring no labeled data
and no parameter updates. This is achieved by
providing instructions in natural language, com-
bined with the self-debiasing method of Schick
y= 1x1=Rick Santorum notches a victory in Kansas caucuses.✓x2=Rick Santorum wins Kansas caucuses.
x1=A man is cutting cucumbers.✓x2=A man is slicing cucumbers.
x1=US closes embassy in Syria✗x2=US Embassy in Syria
x1=A man is playing the cello.✗x2=The cello is playing the man.
x1=A plane is taking off.✗x2=I want to be a pilot.y= 0 .5x1=A woman is seasoning a piece of meat.✓x2=A man is cooking the meat and adding spices [...]
x1=Second day of Egyptian presidential election✓x2=The first night of the election.y= 0x1=A white bus with the word Julia is near water [...]✓x2=There is an open beach in my hometown.
x1=Strong earthquake in Mexico✓x2=It’s the best time to get a job
x1=Closed roads in Armenia✗x2=Open roads in Azerbaijan
x1=The man is playing the guitar.✗x2=I’m not a guitar player.
x1=A man is playing a large flute.✗x2=A man is listening to a small flute.
Table 4: A selection of high-quality ( ✓) and low-quality
(✗) examples in STS -
-x2. Many sentence pairs for
y= 1 are not similar and have quite different meanings.
Some sentence pairs for y= 0 are not on completely
different topics.
et al. (2021). With appropriate measures for han-
dling noisy data, models trained on datasets gener-
ated with DINO achieve strong results on several
semantic textual similarity datasets.
For future work, it would be interesting to see
whether the noise in datasets generated with DINO
can further be reduced, e.g., by using different
sets of instructions (Jiang et al., 2020; Schick and
Schütze, 2021a) or by supplementing our pipeline
with some additional filtering steps.
Acknowledgments This work was funded by the
European Research Council (ERC #740516). We
thank the anonymous reviewers for their helpful
comments.