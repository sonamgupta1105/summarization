Introduction 

While pretrained language models (PLMs) achieve 
strong results for many NLP tasks (Peters et al., 
2018; Radford et al., 2018; Devlin et al., 2019), 
they do not produce good sentence embeddings out 
of the box (Reimers and Gurevych, 2019). Recent 
approaches address this by augmenting or replacing 
the language modeling objective with likewise unsupervised 
sentence-level objectives (e.g., Zhang 
et al., 2020; Li et al., 2020), but they typically 
lag behind their supervised counterparts trained on 
human-annotated sentence pairs. Unfortunately, 
obtaining large amounts of high-quality training 
data can be both difficult and prohibitively expensive 
(Bowman et al., 2015; Agirre et al., 2016). 
Furthermore, with larger and larger model sizes 
(Radford et al., 2019; Raffel et al., 2020; Brown 
et al., 2020; Fedus et al., 2021), it becomes increasingly 
challenging to finetune PLMs. 


(ii) Both input sentence and continuation are generated. 
This does not rely on the availability of any resources. 
To alleviate both problems, we explore a novel 
approach to obtaining high-quality sentence embeddings: 
We mimic the creation of NLI datasets 
by human crowdworkers (Bowman et al., 2015; 
Williams et al., 2018), but replace human annotators 
with large PLMs. This allows us to automatically 
create entire datasets from scratch that can 
be used for supervised training of much smaller 
models. Not only does this solve the problem of 
limited training data, it also provides a viable path 
to leverage big models like GPT-3 (Brown et al., 
2020) without requiring any updates to their parameters. 
As illustrated in Figure 1, our approach is 
based on recent methods for providing instructions 
to PLMs (e.g., Radford et al., 2019; Brown et al., 
2020; Schick and Schütze, 2020, 2021a). We use 
the self-debiasing approach of Schick et al. (2021) 
to ensure that each generated text pair is not only a 


good fit for a given similarity label, but also not a 
good fit for other labels. We refer to our method as 
Datasets from Instructions (DINO). 

In summary, our contributions are as follows: 

• We introduce DINO, a method for automatically 
generating labeled datasets of arbitrary 
size by providing PLMs with instructions. 
• We release STS-(read as “STS-Dino”), the 
first textual similarity dataset generated completely 
automatically, without any human annotation 
effort. 
• We show that Sentence-RoBERTa (Reimers 
and Gurevych, 2019) trained on STS-outperforms 
strong baselines on several semantic 
textual similarity datasets. 
Related 
Work 


There are many unsupervised approaches to obtaining 
sentence embeddings, for example by averaging 
word embeddings (Mikolov et al., 2013; 
Pennington et al., 2014; Bojanowski et al., 2017) or 
with carefully designed sentence-level objectives 
(Le and Mikolov, 2014; Kiros et al., 2015). Ensembling 
several methods improves results (Pörner and 
Schütze, 2019; Pörner et al., 2020). Recent work 
obtains sentence representations by supplementing 
BERT (Devlin et al., 2019) or other PLMs with 
additional unsupervised objectives (Zhang et al., 
2020; Li et al., 2020; Wu et al., 2020; Giorgi et al., 
2020). Often, labeled datasets such as paraphrase 
databases (Wieting and Gimpel, 2018) or natural 
language inference datasets (Conneau et al., 2017; 
Cer et al., 2018; Reimers and Gurevych, 2019) are 
used for supervised learning. 

Some approaches augment existing datasets with 
automatically generated examples (Anaby-Tavor 
et al., 2020; Papanikolaou and Pierleoni, 2020; 
Yang et al., 2020; Mohapatra et al., 2020; Kumar 
et al., 2021), but in contrast to our work, all of 
these approaches require that there already exists a 
labeled dataset for finetuning the generator. Providing 
PLMs with task descriptions for zero-or few-
shot learning has been studied extensively (e.g., 
Radford et al., 2019; Puri and Catanzaro, 2019; 
Brown et al., 2020; Schick and Schütze, 2020, 
2021b,a; Weller et al., 2020; Gao et al., 2021; Tam 
et al., 2021). However, none of these approaches is 
suitable for generating sentence embeddings. 

Closely related to our work, Efrat and Levy 
(2020) examine the ability of PLMs to follow natu-

Task: 
Write 
two 
sentences 
that 
iy. 


Sentence 
1:“x1 
” 


Sentence 
2:“ 


Figure 2: Instruction template Iy(x1) 
for similarity label 
y 
and input sentence x1; iy 
is described in Section 3. 
See Figure 1 for three instantiations of the template. 

ral language instructions for generating examples 
in place of human crowdworkers, but find that their 
approach performs poorly. 

3 
Datasets 
from 
Instructions 


Let M 
be a PLM with vocabulary V 
, X 
= 
V 
∗ 
the set of all token sequences and Y 
a finite set of 
semantic similarity labels. Our aim is to generate a 
dataset Z 
⊂ 
X 
×X 
×Y 
of text pairs (x1, 
x2) 
with 
corresponding similarity labels y. For x 
∈ 
V 
and 
x 
∈ 
X, we denote with pM 
(x 
| 
x) 
the probability 
that M 
assigns to x 
as a continuation of x. 

We first assume that we 
already 
have 
access 
to 
a 
set 
X1 
⊂ 
X 
of 
texts 
(e.g., a set of sentences 
that are typical of the domain of interest). This is a 
realistic setting for many real-world applications, 
where large amounts of unlabeled text are abundant, 
but it is difficult to obtain interesting and (for our 
task) useful text pairs and labels. DINO requires a 
set of instructions I 
= 
{Iy 
| 
y 
∈ 
Y 
} 
where each 
Iy 
∈I 
is a function that, given an input x1 
∈ 
X1, 
prompts its recipient to generate an appropriate 
second text x2. We use the instruction template 
in Figure 2 and consider three levels of similarity 
(Y 
= 
{0, 
0.5, 
1}), where 

 


mean 
the 
same 
thing 
if y 
=1

 


iy 
= 
are 
somewhat 
similar 
if y 
=0.5



 


are 
on 
completely 
different 
topics 
if y 
=0 


is loosely based on Cer et al. (2017)’s five-level 
similarity scheme. Note that for all y, Iy 
ends with 
an opening quotation mark, which allows us to treat 
the first quotation mark generated by the PLM as a 
sign that it is done. 

For a given x1 
∈ 
X1 
and y 
∈ 
Y 
, we could 
directly use the instructions Iy 
to obtain x2 
by continuously 
sampling tokens 

xk 
∼ 
pM 
(xk 
| 
Iy(x1),x1,...,xk−1) 


starting from k 
=1 
until xk 
is a quotation mark 
and setting x2 
= 
x1,...,xk−1. However, we may 


want the PLM to generate a text x2 
that is not only 
a good fit for instruction Iy(x1), but also not a 
good fit for some other instruction Iy 
′ 
(x1). We 

′

refer to y 
as a counterlabel for y 
and denote the 
set of y’s counterlabels as CL(y). For example, 
1 
∈ 
CL(0.5) 
means that for y 
=0.5, we want 
M 
to generate a sentence x2 
that is similar to 
(y 
=0.5), but at the same time does not have the 
same meaning as (y 
=1) sentence x1. We achieve 
this using Schick et al. (2021)’s self-debiasing algorithm: 
When sampling the token xk, we consider 
not just py 
= 
pM 
(xk 
| 
Iy(x1),x1,...,xk−1) 
[xk’s 
probability given Iy(x1)], but also py 
′ 
[xk’s prob


′

ability given Iy 
′ 
(x1)], for all y 
∈ 
CL(y). We 
penalize each token xk 
for which py 
is lower than 
any py 
′ 
by multiplying its probability with a factor 
α 
= 
exp(λ 
· 
δy) 
where 

δy 
= 
py 
− 
max 
py 
′ 
y 
′∈CL(y) 


is the difference between xk’s probability given 
Iy(x1) 
and its maximum probability given Iy 
′ 
(x1) 


′

for any y 
∈ 
CL(y), and the decay constant λ 
is a 
hyperparameter. 

For settings where no 
set 
of 
unlabeled 
texts 
X1 
is 
available, a straightforward approach would be 
to use the phrase shown in Figure 2 up to and including 
the first quotation mark as an instruction 
to let the PLM generate both x1 
and x2. However, 
this approach has at least two issues: First, generated 
texts may not match the required schema (e.g., 
the model may never produce the string “Sentence 
2:”). Second, the set of texts x1 
should ideally be 
highly diverse, whereas we want to give the model 
less leeway when generating x2, so we may want 
to use different sampling strategies for x1 
and x2. 

We solve both problems as follows: We first use 
Iy 
(Figure 2) up to and including the first quotation 
mark (the one right after “Sentence 1:”) to generate 
x1; we stop as soon as the model produces a quotation 
mark. We run this procedure repeatedly until 
we have a sufficient number of sentences. These 
are gathered into a set X1 
and then we proceed 
exactly as in the case where X1 
is already given. 

Experiments 


We evaluate DINO on several English semantic textual 
similarity datasets: the STS tasks 2012–2016 
(Agirre et al., 2012, 2013, 2014, 2015, 2016), the 
STS benchmark (STSb) (Cer et al., 2017), and the 
SICK-Relatedness dataset (SICK) (Marelli et al., 

2014). For all tasks, we adopt the unsupervised 
setting without task-specific training examples. 

We use DINO to generate STS-⊂ 
X×X×Y 
,a 
dataset of text pairs with semantic similarity labels. 
We generate two variants: 

• STS--x2, for which we make use of STSb 
to obtain a set of texts X1; 
• STS--x1x2, where the set of sentences X1 
is generated from scratch. 
We use GPT2-XL as PLM with a decay constant 
of λ 
= 
100 
and the set of counterlabels CL(y)= 


′′ 


{y 
∈ 
Y 
| 
y 
>y}. That is, we do not restrict 
the PLM when generating texts for y 
=1, but for 
y 
=0.5 
(y 
=0) we encourage it not to generate 
texts x2 
that mean the same thing as (are somewhat 
similar to) x1. We apply top-p 
(Holtzman et al., 
2020) and top-k 
(Fan et al., 2018; Holtzman et al., 
2018) sampling with p 
=0.9, k 
=5 
and generate 
up to 40 output tokens. For each x1 
∈ 
X1 
and y 
∈ 
Y 
, we generate up to two corresponding x2’s.2 For 
STS--x1x2, we obtain X1 
by generating 15,000 
sentences using only top-p 
sampling (again with 
p 
=0.9) and no top-k 
sampling to ensure more 
diversity in the generated output. We remove all 
examples where x1 
= 
x2 
(as those provide no 
training signal to the model) and split the datasets 
90/10 into training and validation. 

To assess the quality of the generated datasets, 
we use them to train Sentence-RoBERTa (Reimers 
and Gurevych, 2019), a biencoder architecture 
based on RoBERTa (base) (Liu et al., 2019) that 
measures the similarity of two texts by computing 
the cosine similarity of their embeddings. As 
our datasets contain many noisy examples, we use 
a technique similar to label smoothing (Szegedy 
et al., 2016) and replace similarity scores of 0 
and 
1 
with 0.1 
and 0.9, respectively. Additionally, for 
each x1, we sample two x2’s from other dataset 
entries and augment the dataset with (x1, 
x2, 
0). 
We use the default parameters of Reimers and 
Gurevych (2019) with a batch size of 32 and train 
for at most one epoch; the exact number of training 
steps is determined based on Spearman’s rank 
correlation on the STS-validation set. 

Results 
We compare S-RoBERTa (base) trained 
on datasets generated with DINO to S-BERT and 
S-RoBERTa finetuned on NLI data as well as Universal 
Sentence Encoder (USE) (Cer et al., 2018) 

2As the PLM may not generate a quotation mark in the 
first 40 tokens, we use up to 5 tries to generate the two x2’s. 


unsup. 

sup. 

Model 
UD 
STS12 
STS13 
STS14 
STS15 
STS16 
STSb 
SICK 
Avg. 


InferSent, Glove – 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01 
USE – 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22 
S-BERT (base) – 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89 
S-RoBERTa (base) – 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21 

Avg. GloVe – 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32 
Avg. BERT – 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81 
BERT CLS – 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19 
Zhang et al. (2020) NLI 56.77 69.24 61.21 75.23 70.16 69.21 64.25 66.58 
Li et al. (2020) NLI 59.54 64.69 64.66 72.92 71.84 58.56 65.44 65.38 
Li et al. (2020) STS 63.48 72.14 68.42 73.77 75.37 70.72 63.11 69.57 
DINO (STS--x1x2) – 64.87 78.30 66.38 79.60 76.47 76.51 74.26 
73.77 
DINO (STS--x2) STS 70.27 
81.26 
71.25 
80.49 
77.18 
77.82 
68.09 75.20 


Table 1: Spearman’s rank correlation on STS12–16, STSb and SICK without finetuning on task-specific examples 
for models with NLI supervision (“sup.”) and fully unsupervised (“unsup.”) models using the same evaluation setup 
as Reimers and Gurevych (2019). The second column shows which unlabeled data (“UD”) is used by unsupervised 
approaches in addition to original pretraining data; the final column shows average performance. Results for 
all baselines except Zhang et al. (2020) and Li et al. (2020) are from Reimers and Gurevych (2019). The best 
unsupervised result is shown in bold, the best overall result is underlined. DINO outperforms all unsupervised 
approaches and, surprisingly, also supervised approaches on four out of six STS datasets. 

Model 
STS12-16 
STSb 
SICK 


DINO (STS-
x2) 76.09 
77.82 
68.09 
decay constant λ 
= 
0 
65.50 70.71 67.60 
decay constant λ 
= 
200 
75.40 77.49 66.83 
no label smoothing 74.50 76.26 66.23 
no augmentation 70.90 73.81 63.98 

Table 2: Effect of removing self-debiasing (λ 
=0) 
or increasing the decay constant (λ 
= 
200), using no 
label smoothing and performing no data augmentation 
(sampling random x2’s for each x1) on the performance 
of DINO on STS12-16 (avg), STSb and SICK 

and InferSent (Conneau et al., 2017), all of which 
are trained on hundreds of thousands of labeled text 
pairs from SNLI (Bowman et al., 2015) and MNLI 
(Williams et al., 2018). We additionally compare 
to the following fully unsupervised approaches: averaging 
word-level GloVe (Pennington et al., 2014) 
or BERT (Devlin et al., 2019) embeddings, using 
BERT’s CLS token, and recent methods by Zhang 
et al. (2020) and Li et al. (2020) based on pretrained 
BERT models. We do not compare to approaches 
trained with direct supervision as our focus is on 
obtaining sentence representations without task-
specific labeled examples. As shown in Table 1, 
training on datasets generated with DINO clearly 
outperforms the fully unsupervised baselines; on 
average, training on STS--x2 
even outperforms 
all approaches with NLI supervision. STS--x2 
gives better results than STS--x1x2 
on all STS 
datasets as its examples are – by design – very similar 
to examples found in these datasets, while the 
latter gives better results on SICK. 

We investigate the importance of self-debiasing 
(Schick et al., 2021) in Table 2 (top); as can be 
seen, removing self-debiasing (λ 
=0) dramatically 
hurts performance. Increasing the decay constant 
(λ 
= 
200) leads to slightly worse performance 
as the overall quality of generated sentences decreases 
(Schick et al., 2021). Table 2 (bottom) 
shows that training on STS-requires measures 
to limit the effect of noisy labels: removing label 
smoothing and performing no data augmentation 
(i.e., not generating additional pairs (x1, 
x2, 
0) 
by 
sampling random x2’s for each x1) clearly hurts 
performance. 

To further assess the quality of datasets generated 
with DINO, we additionally perform a small-
scale human evaluation. To this end, we consider 
the exact version of STS--x2 
used for training 
S-RoBERTa; that is, we perform label smoothing, 
augmentation with randomly sampled text pairs, 
and removal of trivial examples where x1 
= 
x2. 
From the resulting dataset, we randomly select 
100 text pairs (x1, 
x2) 
and annotate them ourselves 
with similarity scores y 
∈{0, 
0.1, 
0.5, 
0.9}, where 
we assign a score of 0.9 
when x1 
and x2 
mean 
(almost) the same thing and a score of 0.1 
when 
they are on different topics, but still show a weak 
similarity in some aspect. 

In Table 3, human annotations are compared 
to originally assigned scores, yielding some interesting 
insights. For one, it becomes clear why 
augmentation with randomly sampled text pairs is 
important for good downstream task performance: 
Of the examples generated by DINO that are sup



DINO Labels → 
0.00.10.50.9 
x1 
= 
RickSantorumnotchesavictory inKansas caucuses.✓ 


x2 
= 
Rick Santorum wins Kansas caucuses. 

95%
0.0 


0% 
0% 


x1 
= 
A man is cutting cucumbers. 

0.1 
0% 


11% 
12% 


✓ 


x2 
= 
A man is slicing cucumbers. 

0.5 
5% 


15% 
44% 
41%

Human Labels 

60%
29%
41%
47%
y 
=
0 


y 
=0.5 


y 
=1

x1 
= 
US closes embassy in Syria 

✗

0.9 
0% 
0% 


x2 
= 
US Embassy in Syria 

x1 
= 
A man is playing the cello. 

x2 
= 
The cello is playing the man. 

Table 3: Comparison of similarity scores in STS


-x2 


to human judgments for 100 examples. Examples are 

chosen randomly from the version of STS--x2 
used 

for training (including label smoothing, augmentation 

with random pairs and removal of examples where 

x1 
= 
x2). For column i 
and row j, the value shown is 

the percentage of examples generated by DINO for sim


✗ 


x1 
= 
A plane is taking off. 

x2 
= 
I want to be a pilot. 

✗ 


x1 
= 


A woman is seasoning a piece of meat. 

x2 
= 
A man is cooking the meat and adding spices [...] 

✓ 


Second day of Egyptian presidential election 

x1 
= 


✓

The first night of the election. 

x2 
=

ilarity score i 
that were assigned score j 
in our human 
evaluation. 

x1 
= 
A white bus with the word Julia is near water [...] 
x2 
= 
There is an open beach in my hometown. ✓ 


x1 
= 
Strong earthquake in Mexico 

✓

posed to be on completely different topics, many 

(41%) still have a certain similarity according to 

human judgment. In contrast, randomly sampled 

x2 
= 
It’s the best time to get a job 

x1 
= 
Closed roads in Armenia 

✗ 


x2 
= 
Open roads in Azerbaijan 

pairs are indeed on completely different topics in 
almost all cases. Moreover, we can see that GPT2XL 
has particular difficulty in generating pairs of 
non-identical sentences that really mean the same 
thing: Only 47% of all examples that should have 
the same meaning do actually mean (almost) the 
same thing. However, the strong performance of 
S-RoBERTa trained on STS--x2 
suggests that, 
despite this noise, there is sufficient signal in this 
dataset for successful training. 

We finally take a qualitative look at both positive 
examples where DINO is able to create high-quality 
text pairs and at some typical errors found in many 
of the generated examples. As shown in Table 4, for 
y 
=1 
the PLM sometimes comes up with decent 
paraphrases (e.g. “notches a victory” 7
→ 
“wins”) or 
substitutes with very similar meaning (“cutting” 7
→ 
“slicing”), but more often it generates sentences that 
either omit or mix up important information, and 
sometimes it produces sentences with an entirely 
different meaning. Whereas sentences generated 
for y 
=0.5 
by and large look reasonable, for y 
=0 
the PLM often simply flips words (“closed” 7
→ 
“open”, “large” 7
→ 
“small”) instead of producing 
sentences on completely different topics. 

Conclusion 


We have introduced DINO, a method for using large 
PLMs to generate entire datasets of labeled sentence 
pairs from scratch, requiring no labeled data 
and no parameter updates. This is achieved by 
providing instructions in natural language, combined 
with the self-debiasing method of Schick 

x1 
= 
The man is playing the guitar. 
x2 
= 
I’m not a guitar player. ✗ 


x1 
= 
A man is playing a large flute. 
x2 
= 
A man is listening to a small flute. ✗ 


Table 4: A selection of high-quality (✓) and low-quality 
(✗) examples in STS--x2. Many sentence pairs for 
y 
=1 
are not similar and have quite different meanings. 
Some sentence pairs for y 
=0 
are not on completely 
different topics. 

et al. (2021). With appropriate measures for handling 
noisy data, models trained on datasets generated 
with DINO achieve strong results on several 
semantic textual similarity datasets. 

For future work, it would be interesting to see 
whether the noise in datasets generated with DINO 
can further be reduced, e.g., by using different 
sets of instructions (Jiang et al., 2020; Schick and 
Schütze, 2021a) or by supplementing our pipeline 
with some additional filtering steps.
